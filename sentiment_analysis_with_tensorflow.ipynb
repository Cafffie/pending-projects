{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af20ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4bab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load and preprocess the dataset (replace 'your_dataset.csv' with your actual dataset)\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "df = df[['text', 'sentiment']]  # Assuming your dataset has 'text' and 'sentiment' columns\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # Assuming 3 sentiment classes\n",
    "\n",
    "# Tokenize and encode the text data\n",
    "train_encodings = tokenizer(list(train_data['text']), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_data['text']), truncation=True, padding=True)\n",
    "\n",
    "# Convert to TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    tf.keras.utils.to_categorical(train_data['sentiment'], num_classes=3)  # Assuming 3 sentiment classes\n",
    "))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    tf.keras.utils.to_categorical(test_data['sentiment'], num_classes=3)\n",
    "))\n",
    "\n",
    "# Set up model and optimizer\n",
    "optimizer = AdamW(learning_rate=1e-5)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "model.fit(train_dataset.shuffle(100).batch(8), epochs=5)\n",
    "\n",
    "# Evaluation\n",
    "results = model.evaluate(test_dataset.batch(8))\n",
    "\n",
    "print(f\"Test Accuracy: {results[1]:.4f}\")\n",
    "\n",
    "# Prediction\n",
    "new_text = \"This movie is amazing!\"\n",
    "new_text_encoding = tokenizer([new_text], truncation=True, padding=True, return_tensors='tf')\n",
    "prediction = model.predict(dict(new_text_encoding))\n",
    "\n",
    "predicted_class = tf.argmax(prediction['logits'][0]).numpy()\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ee1da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3133ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "This TensorFlow code demonstrates sentiment analysis using a pre-trained BERT model. \n",
    "It involves loading a pre-trained BERT model, tokenizing the text data, converting to TensorFlow datasets, training the model, \n",
    "evaluating its performance on a test set, and making predictions on new text. \n",
    "Remember to replace 'your_dataset.csv' with the actual path to your dataset and customize the code according to your specific \n",
    "needs.\n",
    "\n",
    "User\n",
    "Show me robust codes to classify text as positive or negative \n",
    "ChatGPT\n",
    "Certainly! Below is an example of a robust text classification using a pre-trained BERT model for sentiment analysis. \n",
    "This example uses TensorFlow and the Hugging Face Transformers library. Make sure to install the required libraries:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "#pip install transformers pandas scikit-learn tensorflow\n",
    "#Now, you can use the following code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load and preprocess the dataset (replace 'your_dataset.csv' with your actual dataset)\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "df = df[['text', 'sentiment']]  # Assuming your dataset has 'text' and 'sentiment' columns\n",
    "\n",
    "# Map sentiment labels to binary (positive/negative)\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # Binary classification\n",
    "\n",
    "# Tokenize and encode the text data\n",
    "train_encodings = tokenizer(list(train_data['text']), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_data['text']), truncation=True, padding=True)\n",
    "\n",
    "# Convert to TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    tf.keras.utils.to_categorical(train_data['sentiment'], num_classes=2)  # Binary classification\n",
    "))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    tf.keras.utils.to_categorical(test_data['sentiment'], num_classes=2)\n",
    "))\n",
    "\n",
    "# Set up model and optimizer\n",
    "optimizer = AdamW(learning_rate=1e-5)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "model.fit(train_dataset.shuffle(100).batch(8), epochs=5)\n",
    "\n",
    "# Evaluation\n",
    "results = model.evaluate(test_dataset.batch(8))\n",
    "\n",
    "print(f\"Test Accuracy: {results[1]:.4f}\")\n",
    "\n",
    "# Prediction\n",
    "new_text = \"This movie is great!\"\n",
    "new_text_encoding = tokenizer([new_text], truncation=True, padding=True, return_tensors='tf')\n",
    "prediction = model.predict(dict(new_text_encoding))\n",
    "\n",
    "predicted_class = tf.argmax(prediction['logits'][0]).numpy()\n",
    "print(f\"Predicted class: {'positive' if predicted_class == 1 else 'negative'}\")\n",
    "This code assumes a binary sentiment classification task (positive/negative). Adjust the number of labels and the mapping based on your specific task. Replace 'your_dataset.csv' with the actual path to your dataset and customize the code according to your needs.\n",
    "\n",
    "User\n",
    "No, I mean code to classify or label a sentence as either positive or negative \n",
    "ChatGPT\n",
    "Certainly! Below is a simple example of a Python script that uses a pre-trained BERT model for sentiment analysis with the Transformers library. This script classifies a sentence as either positive or negative.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load pre-trained sentiment analysis model\n",
    "sentiment_classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "# Example usage\n",
    "sentence = \"I really enjoyed the movie, it was fantastic!\"\n",
    "result = sentiment_classifier(sentence)\n",
    "\n",
    "# Display sentiment classification result\n",
    "label = result[0]['label']\n",
    "confidence = result[0]['score']\n",
    "\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"Sentiment: {label}, Confidence: {confidence:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
